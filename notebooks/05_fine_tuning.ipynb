{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gEpuNFVOWx4"
      },
      "source": [
        "# Fine Tuning de Modelos de Lenguaje\n",
        "\n",
        "[![Abrir en Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aldomunaretto/immune_generative_ai/blob/main/notebooks/05_fine_tuning.ipynb)\n",
        " \n",
        "Este notebook guía paso a paso el proceso de ajuste fino (fine-tuning) de un modelo de lenguaje utilizando la librería Unsloth y Hugging Face. En el realizaremos el ajuste fino (fine tuning) de un modelo de lenguaje para tareas de extracción de información a partir de fragmentos HTML."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ptsAkgjblRL"
      },
      "source": [
        "### Instalación de dependencias en Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7S56C0ZbEwQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, importlib.util\n",
        "!pip install --upgrade -qqq uv\n",
        "if importlib.util.find_spec(\"torch\") is None or \"COLAB_\" in \"\".join(os.environ.keys()):\n",
        "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
        "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
        "    !uv pip install -qqq \\\n",
        "        \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} {get_pil} torchvision bitsandbytes \"transformers==4.56.2\" \\\n",
        "        \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
        "        \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
        "        git+https://github.com/triton-lang/triton.git@0add68262ab0a2e33b84524346cb27cbb2787356#subdirectory=python/triton_kernels\n",
        "elif importlib.util.find_spec(\"unsloth\") is None:\n",
        "    !uv pip install -qqq unsloth\n",
        "!uv pip install --upgrade --no-deps transformers==4.56.2 tokenizers trl==0.22.2 unsloth unsloth_zoo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIjHjIFOb05M"
      },
      "source": [
        "### Instalación de dependencias en entorno local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlNSUlw2cDrv"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q64ip1abYfH"
      },
      "source": [
        "### Importación de librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpX1F9XbVjRO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import unsloth\n",
        "import torch\n",
        "from google.colab import files\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import TrainingArguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZjozby6OWx8"
      },
      "source": [
        "### Verificación de GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOTWElUCWk_v"
      },
      "outputs": [],
      "source": [
        "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No se detecta ninguna GPU'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lOwho9BH5Ok"
      },
      "source": [
        "### Carga del modelo y tokenizador\n",
        " \n",
        "En este paso se descarga y prepara el modelo base (Llama-3.2-3B-Instruct) y su tokenizador usando la función de Unsloth. Se configuran parámetros como la longitud máxima de secuencia y el uso de 4 bits para optimizar memoria y velocidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_v9mEEbWsLgQ"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        "    dtype = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT6Gsc0vcKw2"
      },
      "source": [
        "### Carga del dataset en formato JSON\n",
        " \n",
        "Se carga el dataset que contiene ejemplos de entrada y salida para el ajuste fino. El archivo debe estar en formato JSON y estructurado para tareas de extracción de información."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epntuwQcsMyh"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"json\", data_files=\"json_extraction_dataset_500.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-TcZPM1cMwS"
      },
      "source": [
        "### Definición de la función de formateo del dataset\n",
        " \n",
        "Se crea una función para transformar cada ejemplo del dataset al formato requerido por el modelo. Esto incluye convertir la salida a JSON y estructurar los mensajes en el formato de chat esperado por el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kOzyvsjcYfO"
      },
      "outputs": [],
      "source": [
        "def format_example(row):\n",
        "    # Convertimos el output (diccionario) a string JSON\n",
        "    answer = json.dumps(row[\"output\"], ensure_ascii=False)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Eres un asistente que extrae información de productos a partir de fragmentos HTML.\"},\n",
        "        {\"role\": \"user\", \"content\": row[\"input\"]},\n",
        "        {\"role\": \"assistant\", \"content\": answer},\n",
        "    ]\n",
        "\n",
        "    # Aplicamos el template de chat\n",
        "    row[\"text\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "    return row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q-KbnR7BDfB"
      },
      "source": [
        "### Aplicación del formateo al dataset\n",
        " \n",
        "Se aplica la función de formateo a todos los ejemplos del dataset para que estén listos para el entrenamiento supervisado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdCtaOEGBD_e"
      },
      "outputs": [],
      "source": [
        "dataset = dataset[\"train\"].map(format_example, batched=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPltzuI2cNUB"
      },
      "source": [
        "### Configuración del adaptador LoRA para fine-tuning eficiente\n",
        " \n",
        "Se utiliza la técnica LoRA (Low-Rank Adaptation) para ajustar el modelo de manera eficiente, reduciendo el número de parámetros entrenables y el consumo de memoria. Aquí se configuran los hiperparámetros principales del adaptador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJ4JlqFqsZJ3"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0.0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 42,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1_K2eu_Aslm"
      },
      "source": [
        "### Configuración y ejecución del entrenamiento supervisado (SFTTrainer)\n",
        " \n",
        "Se configura el entrenador supervisado (SFTTrainer) con los hiperparámetros de entrenamiento, el modelo, el tokenizador y el dataset ya formateado. Aquí se define el número de épocas, el tamaño de batch, el optimizador y otros parámetros clave para el fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trVjIivcfryt"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    dataset_text_field=\"text\",\n",
        "    train_dataset = dataset,\n",
        "    args = TrainingArguments(\n",
        "      output_dir = \"outputs\",\n",
        "      per_device_train_batch_size=1,\n",
        "      gradient_accumulation_steps=4,\n",
        "      num_train_epochs=3,\n",
        "      max_steps = 500,\n",
        "      learning_rate=1e-4,\n",
        "      fp16=True,\n",
        "      logging_steps=50,\n",
        "      optim = \"adamw_8bit\",\n",
        "      weight_decay = 0.01,\n",
        "      lr_scheduler_type = \"linear\",\n",
        "      save_steps=500,\n",
        "      save_total_limit=2,\n",
        "      report_to=\"none\"\n",
        "      )\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-A5o7p8BT3O"
      },
      "source": [
        "### Evaluación del modelo afinado\n",
        " \n",
        "Después del entrenamiento, se prueba el modelo con un ejemplo de entrada para verificar que ha aprendido a extraer la información correctamente. Se utiliza un pipeline de generación de texto para obtener la respuesta del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMvzCp8JsirO"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "gen_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=300)\n",
        "\n",
        "prueba_input = \"Extrae información del siguiente producto:\\n<div class='product'><h2>Test Product</h2><span class='price'>$999</span><span class='category'>electronics</span><span class='brand'>OpenAI</span></div>\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Eres un asistente que extrae información de productos a partir de fragmentos HTML.\"},\n",
        "    {\"role\": \"user\", \"content\": prueba_input},\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "generated = gen_pipeline(prompt)[0][\"generated_text\"]\n",
        "print(\"\\n=== Salida generada ===\\n\")\n",
        "print(generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GxXM5OiBXy1"
      },
      "source": [
        "### Guardado del modelo afinado en formato GGUF\n",
        " \n",
        "Se guarda el modelo ajustado en formato GGUF, que es eficiente para su despliegue y uso posterior. También se almacena el tokenizador junto con el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVUrxXE1vY0G"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method=\"fast_quantized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjmsNDLwBcxg"
      },
      "source": [
        "### Descarga del modelo para uso local\n",
        " \n",
        "Si trabajas en Google Colab o en un entorno remoto, este bloque permite descargar el archivo del modelo afinado a tu equipo local para su uso o respaldo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1rIjc3nkxw7"
      },
      "outputs": [],
      "source": [
        "for gguf_file in [f for f in os.listdir() if f.endswith(\".gguf\")]:\n",
        "    print(f\"Downloading {gguf_file}\")\n",
        "    files.download(gguf_file)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
