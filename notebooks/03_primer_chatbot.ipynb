{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e2ef28-594f-4c18-9d22-c6b8cd40ead2",
   "metadata": {},
   "source": [
    "# IA conversacional – ¡también conocido como Chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e39cd8-ec79-4e3e-9c26-5659d42d0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6541d58e-2297-4de1-b1f7-77da1b98b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar\n",
    "\n",
    "openai = OpenAI()\n",
    "MODEL = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16839b5-c03b-4d9d-add6-87a0f6f37575",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"Eres un assistente muy útil\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e97227-f162-4d1a-a0b2-345ff248cbe7",
   "metadata": {},
   "source": [
    "# La estructura interna del historial de mensajes\n",
    "\n",
    "Originalmente, gradio esperaba recibir una función llamada:\n",
    "\n",
    "`chat(message, history)`\n",
    "\n",
    "La cual debía recibir `history` en un formato particular, que debemos asignar al formato OpenAI antes de llamar a OpenAI:\n",
    "\n",
    "```\n",
    "[\n",
    "{\"role\": \"system\", \"content\": \"system message here\"},\n",
    "{\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "{\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "{\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "¡Pero Gradio se ha actualizado! Ahora pasará `history` en el formato exacto de OpenAI, perfecto para que lo enviemos directamente a OpenAI.\n",
    "\n",
    "¡Así que nuestro trabajo se volvió más fácil!\n",
    "\n",
    "Escribiremos una función `chat(message, history)` donde:\n",
    "**message** es el mensaje que se debe usar\n",
    "**history** es la conversación anterior, en formato OpenAI\n",
    "\n",
    "Combinaremos el mensaje del sistema, el historial y el último mensaje, y luego llamaremos a OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eacc8a4-4b48-4358-9e06-ce0020041bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    print(\"El historial es:\")\n",
    "    print(history)\n",
    "    print(\"Y los mensajes son:\")\n",
    "    print(messages)\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334422a-808f-4147-9c4c-57d63d9780d0",
   "metadata": {},
   "source": [
    "## ¡Y entonces entra la magia de Gradio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0866ca56-100a-44ab-8bd0-1568feaf6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f91b414-8bab-472d-b9c9-3fa51259bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"Eres un asistente útil en una tienda de ropa.\n",
    "Debes tratar de alentar gentilmente al cliente a que pruebe los artículos que están en oferta.\n",
    "Los sombreros tienen un 60 % de descuento y la mayoría de los demás artículos tienen un 50 % de descuento.\n",
    "Por ejemplo, si el cliente dice 'Quiero comprar un sombrero',\n",
    "podrías responder algo como 'Genial, tenemos muchos sombreros, incluidos varios que son parte de nuestro evento de rebajas'.\n",
    "Anima al cliente a comprar sombreros si no está seguro de qué comprar.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5be3ec-c26c-42bc-ac16-c39d369883f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e9e4e-7836-43ac-a0c3-e1ab5ed6b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch(inline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f0ffa-55c8-4152-b451-945021676837",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message += \"\\nSi el cliente pide zapatos, debes responder que los zapatos no están en oferta hoy, \\\n",
    "¡pero recuérdale al cliente que mire los sombreros!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c602a8dd-2df7-4eb7-b539-4e01865a6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch(inline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a987a66-1061-46d6-a83a-a30859dc88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    if 'cinturon' in message:\n",
    "        messages.append({\"role\": \"system\", \"content\": \"Para mayor contexto, la tienda no vende cinturones,\\\n",
    "        pero asegúrate de señalar otros artículos en oferta.\"})\n",
    "    \n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20570de2-eaad-42cc-a92c-c779d71b48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch(inline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72578c16",
   "metadata": {},
   "source": [
    "## Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb9e21-df67-4c2b-b952-5e7e7961b03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = ollama.chat(model=\"llama3.2\", messages=messages)\n",
    "    return response['message']['content']\n",
    "\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch(inline=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
