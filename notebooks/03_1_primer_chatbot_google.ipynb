{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e2ef28-594f-4c18-9d22-c6b8cd40ead2",
   "metadata": {},
   "source": [
    "# IA conversacional – ¡también conocido como Chatbot!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf4ac0",
   "metadata": {},
   "source": [
    "## Importar Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e39cd8-ec79-4e3e-9c26-5659d42d0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f47f9c5",
   "metadata": {},
   "source": [
    "## Cargar variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682249c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53801471",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = genai.Client()\n",
    "chat_client = gemini.chats.create(model=\"gemini-3-flash-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eacc8a4-4b48-4358-9e06-ce0020041bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    \n",
    "    # 1. Enviamos el mensaje del usuario al chat\n",
    "    stream = chat_client.send_message_stream(message)\n",
    "    \n",
    "    # 2. Impresión del historial (Debug)\n",
    "    print(\"\\n--- Historial Actualizado ---\")\n",
    "    for message in chat_client.get_history():\n",
    "        print(f'role - {message.role}',end=\": \")\n",
    "        print(message.parts[0].text)\n",
    "        \n",
    "    # 3. Generación de respuesta (Streaming)\n",
    "    partial_text = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.text:\n",
    "            partial_text += chunk.text\n",
    "            yield partial_text\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334422a-808f-4147-9c4c-57d63d9780d0",
   "metadata": {},
   "source": [
    "## ¡Y entonces entra la magia de Gradio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0866ca56-100a-44ab-8bd0-1568feaf6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f91b414-8bab-472d-b9c9-3fa51259bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"Eres un asistente útil en una tienda de ropa.\n",
    "Debes tratar de alentar gentilmente al cliente a que pruebe los artículos que están en oferta.\n",
    "Los sombreros tienen un 60 % de descuento y la mayoría de los demás artículos tienen un 50 % de descuento.\n",
    "Por ejemplo, si el cliente dice 'Quiero comprar un sombrero',\n",
    "podrías responder algo como 'Genial, tenemos muchos sombreros, incluidos varios que son parte de nuestro evento de rebajas'.\n",
    "Anima al cliente a comprar sombreros si no está seguro de qué comprar.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3dbeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_config = types.GenerateContentConfig(\n",
    "    temperature=0.7,\n",
    "    max_output_tokens=1000,\n",
    "    system_instruction=system_message\n",
    ")\n",
    "\n",
    "gemini = genai.Client()\n",
    "chat_client = gemini.chats.create(model=\"gemini-3-flash-preview\", config=generate_config )\n",
    "\n",
    "gr.ChatInterface(fn=chat).launch(inline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f0ffa-55c8-4152-b451-945021676837",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message += \"\\nSi el cliente pide zapatos, debes responder que los zapatos no están en oferta hoy, \\\n",
    "¡pero recuérdale al cliente que mire los sombreros!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c602a8dd-2df7-4eb7-b539-4e01865a6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_config = types.GenerateContentConfig(\n",
    "    temperature=0.7,\n",
    "    max_output_tokens=1000,\n",
    "    system_instruction=system_message\n",
    ")\n",
    "\n",
    "gemini = genai.Client()\n",
    "chat_client = gemini.chats.create(model=\"gemini-3-flash-preview\", config=generate_config )\n",
    "\n",
    "gr.ChatInterface(fn=chat).launch(inline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eb2a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    \n",
    "    # Creamos una variable temporal para lo que enviaremos a Gemini\n",
    "    prompt_a_enviar = message\n",
    "    \n",
    "    # Comprobamos si la palabra clave está en el mensaje (usamos lower() para ignorar mayúsculas)\n",
    "    palabras_clave = [\"cinturon\", \"cinturón\"]\n",
    "    \n",
    "    if any(p in message.lower() for p in palabras_clave):\n",
    "        # Añadimos la instrucción \"del sistema\" al final del mensaje\n",
    "        instruccion_extra = (\n",
    "            \"\\n\\n[INSTRUCCIÓN DEL SISTEMA: Para mayor contexto, la tienda no vende cinturones, \"\n",
    "            \"pero asegúrate de señalar otros artículos en oferta.]\"\n",
    "        )\n",
    "        prompt_a_enviar = message + instruccion_extra\n",
    "\n",
    "    # Nota: Gradio mostrará 'message' (limpio), pero Gemini recibe 'prompt_a_enviar' (con la instrucción)\n",
    "    stream = chat_client.send_message_stream(prompt_a_enviar)\n",
    "    \n",
    "    partial_text = \"\"\n",
    "    for chunk in stream:\n",
    "        if chunk.text:\n",
    "            partial_text += chunk.text\n",
    "            yield partial_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20570de2-eaad-42cc-a92c-c779d71b48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_config = types.GenerateContentConfig(\n",
    "    temperature=0.7,\n",
    "    max_output_tokens=1000,\n",
    "    system_instruction=system_message\n",
    ")\n",
    "\n",
    "gemini = genai.Client()\n",
    "chat_client = gemini.chats.create(model=\"gemini-3-flash-preview\", config=generate_config )\n",
    "\n",
    "gr.ChatInterface(fn=chat).launch(inline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72578c16",
   "metadata": {},
   "source": [
    "## Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb9e21-df67-4c2b-b952-5e7e7961b03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = ollama.chat(model=\"llama3.2\", messages=messages)\n",
    "    return response['message']['content']\n",
    "\n",
    "gr.ChatInterface(fn=chat).launch(inline=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
