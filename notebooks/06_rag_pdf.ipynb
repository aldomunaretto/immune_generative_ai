{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74afe1d2",
   "metadata": {},
   "source": [
    "# Sistema RAG con PDF\n",
    "\n",
    "Este notebook te guiará a través de la construcción de un sistema RAG (Retrieval-Augmented Generation) usando un documento PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e530be",
   "metadata": {},
   "source": [
    "## 1. Importar librerías necesarias\n",
    "\n",
    "En esta sección importamos todas las librerías que utilizaremos para el procesamiento del PDF, la creación de embeddings, la base de datos vectorial y la interacción con el modelo de lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366c8ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864211f9",
   "metadata": {},
   "source": [
    "## 2. Configurar constantes y logging\n",
    "\n",
    "Definimos las rutas de archivos, los nombres de los modelos y configuramos el sistema de logging para poder ver información relevante durante la ejecución del notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d9652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_PATH = \"../data/2312.10997v5.pdf\"  # Ruta al PDF a analizar\n",
    "MODEL_NAME = \"llama3.2\"      # Nombre del modelo de lenguaje\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"  # Modelo de embeddings\n",
    "VECTOR_STORE_NAME = \"rag_simple\"      # Nombre de la colección vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a7aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ec102c",
   "metadata": {},
   "source": [
    "## 3. Cargar y visualizar el PDF\n",
    "\n",
    "En este paso cargamos el documento PDF y mostramos un resumen o las primeras líneas para que puedas ver el contenido que se va a procesar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba38f8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_pdf(doc_path):\n",
    "    \"\"\"Cargar documentos PDF.\"\"\"\n",
    "    if os.path.exists(doc_path):\n",
    "        loader = UnstructuredPDFLoader(file_path=doc_path)\n",
    "        data = loader.load()\n",
    "        logging.info(\"PDF cargado correctamente.\")\n",
    "        return data\n",
    "    else:\n",
    "        logging.error(f\"No se encontró el archivo PDF en la ruta: {doc_path}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f868a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ingest_pdf(DOC_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1087bef",
   "metadata": {},
   "source": [
    "## 4. Dividir el documento en fragmentos\n",
    "\n",
    "Dividimos el texto del PDF en fragmentos más pequeños (chunks) para facilitar la búsqueda y el procesamiento posterior. Mostramos algunos ejemplos de estos fragmentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66562ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents):\n",
    "    \"\"\"Divide documentos en fragmentos más pequeños.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=300)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    logging.info(f\"Documentos divididos en {len(chunks)} fragmentos.\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cca5527",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e4e67b",
   "metadata": {},
   "source": [
    "## 5. Crear la base de datos vectorial\n",
    "\n",
    "Convertimos los fragmentos en vectores numéricos (embeddings) y los almacenamos en una base de datos vectorial. Esto permite realizar búsquedas semánticas eficientes sobre el contenido del PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44e6971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_db(chunks):\n",
    "    \"\"\"Crear una base de datos vectorial a partir de fragmentos de documentos.\"\"\"\n",
    "    vector_db = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=OllamaEmbeddings(model=EMBEDDING_MODEL),\n",
    "        collection_name=VECTOR_STORE_NAME,\n",
    "        persist_directory=\"../chroma_db\",\n",
    "    )\n",
    "    logging.info(\"Base de datos vectorial creada.\")\n",
    "    return vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fed4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = create_vector_db(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea778ec",
   "metadata": {},
   "source": [
    "## 6. Inicializar el modelo de lenguaje\n",
    "\n",
    "Cargamos el modelo de lenguaje que se usará para generar respuestas y para crear queries alternativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b104de",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab0aebc",
   "metadata": {},
   "source": [
    "## 7. Crear el recuperador de información (retriever)\n",
    "\n",
    "Configuramos el recuperador multi-query, que genera varias versiones de una pregunta para mejorar la recuperación de información relevante del PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cfb392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(vector_db, llm):\n",
    "    \"\"\"Crear un recuperador básico.\"\"\"\n",
    "    # Usamos el retriever básico configurado para devolver los 5 documentos más relevantes\n",
    "    retriever = vector_db.as_retriever(search_kwargs={\"k\": 5})\n",
    "    logging.info(\"Retriever creado.\")\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2663cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = create_retriever(vector_db, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860bfcee",
   "metadata": {},
   "source": [
    "## 8. Construir la cadena RAG\n",
    "\n",
    "Enlazamos el recuperador, el modelo de lenguaje y el parser de salida en una cadena que implementa el flujo RAG completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce15b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chain(retriever, llm):\n",
    "    \"\"\"Crear la cadena\"\"\"\n",
    "    template = \"\"\"Responde la pregunta basándote ÚNICAMENTE en el siguiente contexto:\\n{context}\\nPregunta: {question}\\n\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    logging.info(\"Cadena RAG creada exitosamente.\")\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c7baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_chain(retriever, llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5f7d59",
   "metadata": {},
   "source": [
    "## 9. Realizar una consulta de ejemplo y mostrar la respuesta\n",
    "\n",
    "Ejecutamos una consulta de ejemplo sobre el sistema RAG y mostramos la respuesta generada, explicando cada paso del proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249efddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pregunta = \"¿Que es RAG?\"\n",
    "print(f\"\\nConsulta de ejemplo: {pregunta}\\n\")\n",
    "respuesta = chain.invoke(input=pregunta)\n",
    "print(\"Respuesta generada por el sistema:\")\n",
    "display(Markdown(respuesta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617ef758",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## 10. Interfaz interactiva para consultas\n",
    "\n",
    "Le damos una vuelta a nuestro codigo para hacerlo interactivo. En esta celda te permite preguntar de manera interactiva utilizando widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348a5d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pregunta = input(\"Introduce tu pregunta: \")\n",
    "respuesta = chain.invoke(input=pregunta)\n",
    "\n",
    "out = widgets.Output()\n",
    "with out:\n",
    "    display(Markdown(respuesta))\n",
    "\n",
    "display(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe1ddec",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## 11. Función principal interactiva\n",
    "\n",
    "Otra mejora es definir una función principal (`main`) que permita ejecutar toda la cadena RAG y realizar preguntas sobre su contenido utilizando widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b82375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data = ingest_pdf(DOC_PATH)\n",
    "\n",
    "    chunks = split_documents(data)\n",
    "\n",
    "    vector_db = create_vector_db(chunks)\n",
    "\n",
    "    llm = ChatOllama(model=MODEL_NAME)\n",
    "\n",
    "    retriever = create_retriever(vector_db, llm)\n",
    "\n",
    "    chain = create_chain(retriever, llm)\n",
    "    \n",
    "    pregunta = input(\"Introduce tu pregunta: \")\n",
    "    respuesta = chain.invoke(input=pregunta)\n",
    "\n",
    "    out = widgets.Output()\n",
    "    with out:\n",
    "        display(Markdown(respuesta))\n",
    "\n",
    "    display(out)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9f36c2",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## 11. Interfaz interactiva avanzada\n",
    "\n",
    "En esta última celda se implementa una interfaz interactiva mejorada que permite seleccionar dinámicamente el archivo PDF y realizar preguntas sobre su contenido. Utiliza widgets para facilitar la selección del documento y la introducción de consultas, mostrando las respuestas generadas por el sistema RAG de manera clara y accesible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8f9db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data_dir = \"../data\"\n",
    "    pdf_files = [f for f in os.listdir(data_dir) if f.lower().endswith(\".pdf\")]\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(\"No se encontraron archivos PDF en la carpeta ../data\")\n",
    "        return\n",
    "\n",
    "    file_selector = widgets.Dropdown(\n",
    "        options=pdf_files,\n",
    "        description='PDF:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    pregunta_box = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Introduce tu pregunta',\n",
    "        description='Pregunta:',\n",
    "        disabled=False\n",
    "    )\n",
    "    ejecutar_btn = widgets.Button(\n",
    "        description=\"Ejecutar\",\n",
    "        button_style='success'\n",
    "    )\n",
    "    output = widgets.Output()\n",
    "\n",
    "    display(file_selector, pregunta_box, ejecutar_btn, output)\n",
    "\n",
    "    def on_ejecutar_clicked(b):\n",
    "        output.clear_output()\n",
    "        with output:\n",
    "            selected_path = os.path.join(data_dir, file_selector.value)\n",
    "            data = ingest_pdf(selected_path)\n",
    "            if data is None:\n",
    "                print(\"No se pudo cargar el PDF.\")\n",
    "                return\n",
    "            chunks = split_documents(data)\n",
    "            vector_db = create_vector_db(chunks)\n",
    "            llm = ChatOllama(model=MODEL_NAME)\n",
    "            retriever = create_retriever(vector_db, llm)\n",
    "            chain = create_chain(retriever, llm)\n",
    "            pregunta = pregunta_box.value\n",
    "            if not pregunta.strip():\n",
    "                print(\"Por favor, introduce una pregunta.\")\n",
    "                return\n",
    "            respuesta = chain.invoke(input=pregunta)\n",
    "            display(Markdown(respuesta))\n",
    "\n",
    "    ejecutar_btn.on_click(on_ejecutar_clicked)\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d06b82",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "## 12. Interfaz interactiva con síntesis de voz\n",
    "\n",
    "En esta celda, la interfaz interactiva permite seleccionar un archivo PDF, realizar preguntas sobre su contenido y escuchar la respuesta generada mediante síntesis de voz utilizando ElevenLabs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f46c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data_dir = \"../data\"\n",
    "    pdf_files = [f for f in os.listdir(data_dir) if f.lower().endswith(\".pdf\")]\n",
    "    \n",
    "    if not pdf_files:\n",
    "        print(\"No se encontraron archivos PDF en la carpeta ../data\")\n",
    "        return\n",
    "\n",
    "    file_selector = widgets.Dropdown(\n",
    "        options=pdf_files,\n",
    "        description='PDF:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    pregunta_box = widgets.Text(\n",
    "        value='',\n",
    "        placeholder='Introduce tu pregunta',\n",
    "        description='Pregunta:',\n",
    "        disabled=False\n",
    "    )\n",
    "    ejecutar_btn = widgets.Button(\n",
    "        description=\"Ejecutar\",\n",
    "        button_style='success'\n",
    "    )\n",
    "    output = widgets.Output()\n",
    "\n",
    "    display(file_selector, pregunta_box, ejecutar_btn, output)\n",
    "\n",
    "    def on_ejecutar_clicked(b):\n",
    "        output.clear_output()\n",
    "        with output:\n",
    "            selected_path = os.path.join(data_dir, file_selector.value)\n",
    "            data = ingest_pdf(selected_path)\n",
    "            if data is None:\n",
    "                print(\"No se pudo cargar el PDF.\")\n",
    "                return\n",
    "            chunks = split_documents(data)\n",
    "            vector_db = create_vector_db(chunks)\n",
    "            llm = ChatOllama(model=MODEL_NAME)\n",
    "            retriever = create_retriever(vector_db, llm)\n",
    "            chain = create_chain(retriever, llm)\n",
    "            pregunta = pregunta_box.value\n",
    "            if not pregunta.strip():\n",
    "                print(\"Por favor, introduce una pregunta.\")\n",
    "                return\n",
    "            respuesta = chain.invoke(input=pregunta)\n",
    "            display(Markdown(respuesta))\n",
    "            \n",
    "            # === TEXT TO SPEECH ===\n",
    "            from elevenlabs.client import ElevenLabs\n",
    "            from elevenlabs import stream\n",
    "            from dotenv import load_dotenv\n",
    "\n",
    "            load_dotenv()\n",
    "            api_key = os.getenv(\"ELEVENLABS_API_KEY\")\n",
    "            if api_key:\n",
    "                client = ElevenLabs(api_key=api_key)\n",
    "                audio_stream = client.text_to_speech.stream(\n",
    "                    text=respuesta, \n",
    "                    voice_id=\"JBFqnCBsd6RMkjVDRZzb\",\n",
    "                    model_id=\"eleven_multilingual_v2\"\n",
    "                )\n",
    "                stream(audio_stream)\n",
    "            else:\n",
    "                print(\"No se encontró la API KEY de ElevenLabs.\")\n",
    "\n",
    "    ejecutar_btn.on_click(on_ejecutar_clicked)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13f4f68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
