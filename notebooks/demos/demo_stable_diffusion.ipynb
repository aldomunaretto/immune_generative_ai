{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5ab5be2",
   "metadata": {},
   "source": [
    "~~~python\n",
    "from huggingface_hub import login\n",
    "login(token=\"\")\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd66cc1",
   "metadata": {},
   "source": [
    "~~~python\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"\")\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab0ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2241967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float32\n",
    ").to(\"mps\") # Usa \"cpu\", \"cuda\" para GPU o \"mps\" para Apple Silicon\n",
    "\n",
    "prompt = \"a futuristic city at sunset, ultra-detailed, 4K\"\n",
    "image = pipe(prompt).images[0]\n",
    "#image.save(\"output_sb1-5.png\") # Guarda la imagen generada\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c6307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float32\n",
    ").to(\"mps\")\n",
    "prompt = \"a futuristic city at sunset, ultra-detailed, 4K\"\n",
    "generator = torch.manual_seed(42)\n",
    "image = pipe(prompt, generator=generator).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c07446",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=torch.float32\n",
    ").to(\"mps\")\n",
    "prompt = \"a futuristic city at sunset, ultra-detailed, 4K\"\n",
    "generator = torch.manual_seed(42)\n",
    "image = pipe(prompt, generator=generator).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf99204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "\n",
    "pipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\", torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"mps\")\n",
    "\n",
    "image = pipe(\n",
    "    \"A cat holding a sign that says hello world\",\n",
    "    negative_prompt=\"\",\n",
    "    num_inference_steps=28,\n",
    "    guidance_scale=7.0,\n",
    "    generator=torch.manual_seed(42)\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9987c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "\n",
    "pipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\", torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"mps\")\n",
    "\n",
    "image = pipe(\n",
    "    \"A cat holding a sign that says hello world\",\n",
    "    negative_prompt=\"\",\n",
    "    num_inference_steps=28,\n",
    "    guidance_scale=7.0,\n",
    "    generator=torch.manual_seed(42)\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36e807a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "\n",
    "pipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\", torch_dtype=torch.float16)\n",
    "pipe.to(\"mps\")\n",
    "\n",
    "image = pipe(\n",
    "    prompt=\"a photo of a cat holding a sign that says hello world\",\n",
    "    negative_prompt=\"blurry, low quality, text, dog\",\n",
    "    num_inference_steps=28,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    guidance_scale=7.0,\n",
    "    generator=torch.manual_seed(42)\n",
    ").images[0]\n",
    "# image.save(\"sd3_hello_world.png\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733a3f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f18debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio Generation\n",
    "\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "from transformers import pipeline\n",
    "from diffusers import DiffusionPipeline\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio\n",
    "\n",
    "synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\", device='cpu')\n",
    "\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "\n",
    "speech = synthesiser(\"¡Hola a un ingeniero en inteligencia artificial, en camino hacia la maestría!\",\n",
    "                     forward_params={\"speaker_embeddings\": speaker_embedding})\n",
    "\n",
    "sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])\n",
    "Audio(\"speech.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a291be32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio Generation\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio\n",
    "\n",
    "synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\", device='cpu')\n",
    "\n",
    "# Usar un embedding de voz del dataset actualizado\n",
    "embeddings_dataset = load_dataset(\"anton-l/ecapa-tdnn-voxceleb-embeddings\", split=\"train\")\n",
    "speaker_embedding = torch.tensor(embeddings_dataset[0][\"embedding\"]).unsqueeze(0)\n",
    "\n",
    "speech = synthesiser(\n",
    "    \"¡Hola a un ingeniero en inteligencia artificial, en camino hacia la maestría!\",\n",
    "    forward_params={\"speaker_embeddings\": speaker_embedding}\n",
    ")\n",
    "\n",
    "sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])\n",
    "Audio(\"speech.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f525d178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio Generation\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "from IPython.display import Audio\n",
    "\n",
    "synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\", device='cpu')\n",
    "speech = synthesiser(\"¡Hola a un ingeniero en inteligencia artificial, en camino hacia la maestría!\")\n",
    "sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])\n",
    "Audio(\"speech.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06caa4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fairseq sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433eda1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\n",
    "from fairseq.models.text_to_speech.hub_interface import TTSHubInterface\n",
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "models, cfg, task = load_model_ensemble_and_task_from_hf_hub(\n",
    "    \"facebook/tts_transformer-es-css10\",\n",
    "    arg_overrides={\"vocoder\": \"hifigan\", \"fp16\": False}\n",
    ")\n",
    "model = models[0]\n",
    "TTSHubInterface.update_cfg_with_data_cfg(cfg, task.data_cfg)\n",
    "generator = task.build_generator([model], cfg)\n",
    "\n",
    "text = \"Hola, esta es una prueba.\"\n",
    "\n",
    "sample = TTSHubInterface.get_model_input(task, text)\n",
    "wav, rate = TTSHubInterface.get_prediction(task, model, generator, sample)\n",
    "\n",
    "ipd.Audio(wav, rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143a5572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import scipy\n",
    "\n",
    "synthesiser = pipeline(\"text-to-speech\", \"suno/bark\", device='cpu')\n",
    "\n",
    "speech = synthesiser(\"Hello, my dog is cooler than you!\", forward_params={\"do_sample\": True})\n",
    "\n",
    "scipy.io.wavfile.write(\"bark_out.wav\", rate=speech[\"sampling_rate\"], data=speech[\"audio\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af80093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import scipy\n",
    "\n",
    "synthesiser = pipeline(\"text-to-speech\", \"suno/bark\", device='cpu')\n",
    "\n",
    "speech = synthesiser(\"Hello, my dog is cooler than you!\", forward_params={\"do_sample\": True})\n",
    "\n",
    "# Convierte a float32 para evitar el error\n",
    "audio = speech[\"audio\"].astype(\"float32\")\n",
    "scipy.io.wavfile.write(\"bark_out.wav\", rate=speech[\"sampling_rate\"], data=audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30683c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from kokoro import KPipeline\n",
    "from IPython.display import display, Audio\n",
    "import soundfile as sf\n",
    "import torch\n",
    "pipeline = KPipeline(lang_code='a')\n",
    "text = '''\n",
    "[Kokoro](/kˈOkəɹO/) is an open-weight TTS model with 82 million parameters. Despite its lightweight architecture, it delivers comparable quality to larger models while being significantly faster and more cost-efficient. With Apache-licensed weights, [Kokoro](/kˈOkəɹO/) can be deployed anywhere from production environments to personal projects.\n",
    "'''\n",
    "generator = pipeline(text, voice='af_heart')\n",
    "for i, (gs, ps, audio) in enumerate(generator):\n",
    "    print(i, gs, ps)\n",
    "    display(Audio(data=audio, rate=24000, autoplay=i==0))\n",
    "    sf.write(f'{i}.wav', audio, 24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dc13aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from kokoro import KPipeline\n",
    "from IPython.display import display, Audio\n",
    "import soundfile as sf\n",
    "import torch\n",
    "pipeline = KPipeline(lang_code='e')\n",
    "text = '''\n",
    "Cada vez que pienso en ti, mis ojos rompen en llanto; y muy triste me pregunto, ¿por qué te quiero tanto?\n",
    "'''\n",
    "generator = pipeline(text, voice='af_heart')\n",
    "for i, (gs, ps, audio) in enumerate(generator):\n",
    "    print(i, gs, ps)\n",
    "    display(Audio(data=audio, rate=24000, autoplay=i==0))\n",
    "    sf.write(f'{i}.wav', audio, 24000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
