# Immune Generative AI

Este repositorio tiene como objetivo introducir los fundamentos de la **Inteligencia Artificial Generativa (GenAI)** y proporcionar una guÃ­a prÃ¡ctica para trabajar con **Modelos de Lenguaje de Gran TamaÃ±o (LLMs)** usando herramientas de cÃ³digo abierto como **Ollama**.

---

## ğŸ“‘ Ãndice de Contenido

- [ğŸ§  Â¿QuÃ© es la Inteligencia Artificial Generativa?](#-quÃ©-es-la-inteligencia-artificial-generativa)
- [ğŸ“š Â¿QuÃ© son los LLMs?](#-quÃ©-son-los-llms)
- [ğŸ¢ Proveedores Comerciales de LLMs](#-proveedores-comerciales-de-llms)
- [ğŸŒ Modelos de CÃ³digo Abierto (actualizado 2025)](#-modelos-de-cÃ³digo-abierto-actualizado-2025)
- [ğŸ“ Context Engineering](#-context-engineering)
- [ğŸ¯ Fine-tuning](#-fine-tuning)
- [ğŸ“‚ Retrieval-Augmented Generation (RAG)](#-retrieval-augmented-generation-rag)
- [ğŸ¤— Hugging Face](#-hugging-face)
- [ğŸ§° Ollama](#-ollama)
- [ğŸ–¥ï¸ LM Studio](#-lm-studio)
- [ğŸ§© AnythingLLM](#-anythingllm)
- [ğŸ“š Msty](#-msty)
- [ğŸŒ Gradio](#-gradio)
- [ğŸ“Š Streamlit](#-streamlit)
- [ğŸ—£ï¸ ElevenLabs](#ï¸-elevenlabs)
- [ğŸ”— Recursos adicionales](#-recursos-adicionales)

---

## ğŸ§  Â¿QuÃ© es la Inteligencia Artificial Generativa?

La **IA Generativa** es un campo de la inteligencia artificial que se centra en la creaciÃ³n de contenido nuevo y original a partir de datos existentes. Esto incluye texto, imÃ¡genes, cÃ³digo, mÃºsica, video y mÃ¡s. Su auge reciente se debe al desarrollo de modelos de deep learning capaces de generar resultados sorprendentes en tareas creativas y cognitivas.

---

## ğŸ“š Â¿QuÃ© son los LLMs?

Los **Large Language Models (LLMs)** son redes neuronales entrenadas con grandes cantidades de texto para aprender patrones del lenguaje humano. Estos modelos pueden:

- Generar texto coherente
- Resumir documentos
- Traducir entre idiomas
- Contestar preguntas
- Escribir cÃ³digo, entre muchas otras tareas.

---

## ğŸ¢ Proveedores Comerciales de LLMs

A continuaciÃ³n se listan algunos de los principales proveedores que ofrecen LLMs accesibles mediante API o servicios cloud:

| Proveedor       | Modelo Principal            | Plataforma/API                   |
|------------------|-----------------------------|----------------------------------|
| **OpenAI**       | GPT-4o, GPT-4, GPT-3.5       | https://platform.openai.com     |
| **Anthropic**    | Claude 3                    | https://www.anthropic.com       |
| **Google**       | Gemini 1.5                  | https://ai.google.dev           |
| **Meta**         | LLaMA 4                     | https://ai.meta.com/llama       |
| **Alibaba**      | Qwen 2                      | https://qwenlm.github.io        |
| **DeepSeek AI**  | DeepSeek-V2, DeepSeek-Coder | https://deepseekcoder.github.io |
| **Mistral**      | Mistral/Mixtral             | https://mistral.ai               |
| **Cohere**       | Command-R+                  | https://cohere.com               |
| **Amazon**       | Titan                       | https://aws.amazon.com/bedrock  |

---

## ğŸŒ Modelos de CÃ³digo Abierto (actualizado 2025)

A continuaciÃ³n se listan algunos de los LLMs open source mÃ¡s destacados y actualizados:

| Modelo           | Autor/OrganizaciÃ³n | TamaÃ±os Disponibles | VersiÃ³n Actual | Licencia      |
|------------------|--------------------|----------------------|----------------|----------------|
| **LLaMA**        | Meta               | 8B, 70B              | LLaMA 4        | Meta RAIL      |
| **Qwen**         | Alibaba            | 0.5B a 110B          | Qwen 2         | Apache 2.0     |
| **DeepSeek**     | DeepSeek AI        | 1.3B a 236B          | DeepSeek-V2    | MIT            |
| **Phi**          | Microsoft          | 3.8B, 7B             | Phi-3          | MIT            |
| **Gemma**        | Google             | 2B, 7B               | Gemma 1.1      | Apache 2.0     |
| **Mistral**      | Mistral AI         | 7B, Mixtral (12.7B MoE) | Mistral 7B / Mixtral 8x7B | Apache 2.0 |
| **Falcon**       | TII (UAE)          | 7B, 180B             | Falcon 180B    | Apache 2.0     |
| **Command-R**    | Cohere             | 35B                  | Command-R+     | RAIL           |

---

## ğŸ“ Context Engineering

El **Context Engineering** consiste en diseÃ±ar cuidadosamente los *prompts* y la informaciÃ³n de entrada para optimizar las respuestas de un LLM sin necesidad de modificar sus pesos. 
Se centra en:
- EstructuraciÃ³n de *prompts* y *templates*.
- InserciÃ³n de *context windows* con ejemplos o instrucciones previas.
- Uso de tÃ©cnicas como **Chain-of-Thought (CoT)**, **Few-Shot Prompting** y **ReAct**.

ğŸ“– **Lecturas sobre Context Engineering**:
- [A Survey of Context Engineering for Large Language Models](https://arxiv.org/abs/2507.13334)
- [Context Rot: How Increasing Input Tokens Impacts LLM Performance](https://research.trychroma.com/context-rot)
- [How Long Contexts Fail](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html)
- [Why â€œContext Engineeringâ€ Matters](https://www.dbreunig.com/2025/07/24/why-the-term-context-engineering-matters.html)
- [Optimizing LangChain AI Agents with Contextual Engineering](https://levelup.gitconnected.com/optimizing-langchain-ai-agents-with-contextual-engineering-0914d84601f3)

ğŸ”§ **LibrerÃ­as comunes**:
- [DSPy](https://dspy.ai/) (Framework para context engineering basado en programas declarativos)
    - [ğŸ™ GitHub Repository](https://github.com/stanfordnlp/dspy) 
- [Guidance](https://github.com/microsoft/guidance)
- [Promptify](https://github.com/promptslab/Promptify)

---

## ğŸ¯ Fine-tuning

El **Fine-tuning** es el proceso de ajustar los parÃ¡metros de un modelo previamente entrenado usando un conjunto de datos especÃ­fico para una tarea concreta. 
Se utiliza para:
- Mejorar rendimiento en dominios especializados.
- Adaptar el estilo o formato de salida.
- Crear *instruction-tuned models* para casos concretos.

### ğŸ”§ LibrerÃ­as y frameworks:
- [Hugging Face Transformers](https://huggingface.co/transformers)
- [PEFT](https://github.com/huggingface/peft) (Parameter Efficient Fine-Tuning)
- [LoRA](https://github.com/microsoft/LoRA)
- [TRL (Transformer Reinforcement Learning)](https://github.com/huggingface/trl)
- [H2O LLM Studio](https://github.com/h2oai/h2o-llmstudio)
- [ğŸ¦¥ Unsloth](https://unsloth.ai/) (Framework optimizado para fine-tuning rÃ¡pido y eficiente)
    - [ğŸ™ GitHub Repository](https://github.com/unslothai/unsloth)
    - [ğŸ¤— Hugging Face Organization Card](https://huggingface.co/unsloth)
- [ğŸ¦™ LLaMA-Factory](https://llamafactory.readthedocs.io/en/latest/)
    - [ğŸ™ GitHub Repository](https://github.com/hiyouga/LLaMA-Factory)

### ğŸª¢ Datasets para entrenamiento y ajuste fino (Fine-Tuning) de LLMs:
- [The Pile](https://pile.eleuther.ai/)
- [HelpSteer: Helpfulness SteerLM Dataset](https://huggingface.co/datasets/nvidia/HelpSteer)
- [No Robots](https://huggingface.co/datasets/HuggingFaceH4/no_robots)
- [Anthropic_HH_Golden](https://huggingface.co/datasets/Unified-Language-Model-Alignment/Anthropic_HH_Golden)
- [Trelis Function Calling Dataset](https://huggingface.co/datasets/Trelis/function_calling_extended)
- [Dolma](https://huggingface.co/datasets/allenai/dolma)
- [Open-Platypus](https://huggingface.co/datasets/garage-bAInd/Open-Platypus)
- [Puffin](https://huggingface.co/datasets/LDJnr/Puffin)

### ğŸ“„ Formatos para Fine Tuning: Alpaca vs ShareGPT

Los formatos **Alpaca** y **ShareGPT** son los mÃ¡s utilizados para el fineâ€‘tuning supervisado de LLMs, especialmente con frameworks como **LLaMAâ€‘Factory** y **Unsloth**.

#### ğŸ§ª Formato Alpaca

ğŸ“‚ Estructura tÃ­pica (JSON)

```json
{
  "instruction": "...",    
  "input": "...",          
  "output": "...",         
  "system": "...",         
  "history": [             
    ["instrucciÃ³n anterior", "respuesta anterior"]
  ]
}
```

- `instruction`: Pregunta o instrucciÃ³n del humano (requerido).
- `input`: Contexto adicional (opcional).
- `output`: Respuesta esperada del modelo (requerido).
- `system`: Mensaje del sistema (opcional).
- `history`: Rondas anteriores en conversaciones multironda (opcional).

##### âœ… Ventajas
- Muy simple y ampliamente adoptado.
- Ideal para datasets de instrucciÃ³n-respuesta de una sola ronda.

##### âš ï¸ Limitaciones
- No tiene estructura nativa para roles mÃºltiples o multironda avanzada.
- Se apoya en tokens de separaciÃ³n (`###` o EOS).


#### ğŸ§µ ShareGPT

##### ğŸ“‚ Estructura tÃ­pica (JSON)

```json
{
  "conversations": [
    { "from": "human", "value": "â€¦instrucciÃ³n humanaâ€¦" },
    { "from": "gpt",   "value": "â€¦respuesta del modeloâ€¦" },
    { "from": "function_call", "value": "â€¦argumentos de herramientaâ€¦" },
    { "from": "observation",   "value": "â€¦resultado de herramientaâ€¦" }
  ],
  "system": "...",
  "tools": "..."
}
```

- Permite mÃºltiples roles: `human`, `gpt`, `function_call`, `observation`.
- DiseÃ±ado para conversaciones multironda y llamadas a funciones.

##### âœ… Ventajas
- Soporta roles y contextos complejos.
- Ideal para datasets de diÃ¡logo real y herramientas.

##### âš ï¸ Limitaciones
- MÃ¡s complejo de preparar que Alpaca Format.


#### ğŸ“¦ Soporte en Frameworks

- **LLaMAâ€‘Factory**: Soporta ambos formatos mediante configuraciÃ³n en `dataset_info.json`.
- **Unsloth**:
  - Permite convertir con `standardize_sharegpt()`.
  - Ofrece `conversation_extension` para simular multironda desde Alpaca.


#### ğŸ“Š Comparativa RÃ¡pida

| Aspecto                  | Alpaca Format                         | ShareGPT Format                              |
|--------------------------|----------------------------------------|----------------------------------------------|
| InstrucciÃ³n + respuesta  | âœ…                                    | âœ…                                           |
| Multi-turno              | âš ï¸ Opcional, sin estructura fija       | âœ… Nativo                                    |
| Roles adicionales        | âŒ                                     | âœ… function_call, observation, etc.          |
| Complejidad              | ğŸ”¹ Baja                               | ğŸ”¹ Media-Alta                                |
| ConversiÃ³n disponible    | ğŸ”¸ Limitada                            | âœ… Via Unsloth                               |


#### ğŸ§  Â¿CuÃ¡l elegir?

- **Alpaca** â†’ Para datasets simples de instrucciones/respuestas.
- **ShareGPT** â†’ Para diÃ¡logos multironda, roles mÃºltiples y escenarios con herramientas.

---

## ğŸ“‚ Retrieval-Augmented Generation (RAG)

El **RAG** combina la generaciÃ³n de texto con la recuperaciÃ³n de informaciÃ³n externa en tiempo real. 
En lugar de confiar solo en el conocimiento interno del LLM, **recupera documentos relevantes** y los pasa como contexto al modelo antes de generar la respuesta.

ğŸ”§ **LibrerÃ­as comunes**:
- [LangChain](https://www.langchain.com/)
- [LangGraph](https://www.langgraph.dev/) 
- [LlamaIndex](https://www.llamaindex.ai/)
- [Haystack](https://haystack.deepset.ai/)
- [Pinecone](https://www.pinecone.io/) (vector DB)
- [Weaviate](https://weaviate.io/) (vector DB)
- [Chroma](https://www.trychroma.com/) (vector DB)

**Casos de uso**:
- Chatbots empresariales con documentos privados.
- BÃºsqueda semÃ¡ntica combinada con LLMs.
- Sistemas de soporte y asistencia con informaciÃ³n actualizada.

---

## ğŸ¤— Hugging Face

[Hugging Face](https://huggingface.co/) es una de las plataformas mÃ¡s influyentes en el ecosistema de inteligencia artificial moderna. Su propÃ³sito es democratizar el acceso a modelos de IA, datasets y herramientas para investigaciÃ³n, desarrollo y despliegue de soluciones basadas en machine learning.

**Â¿QuÃ© ofrece Hugging Face?**
- Un *hub centralizado* de modelos entrenados y datasets etiquetados.
- LibrerÃ­as como `transformers`, `datasets`, `peft`, `diffusers` y `evaluate`.
- Espacios (*Spaces*) para ejecutar y compartir demos interactivas.
- La Hugging Face Hub API para integrar modelos en producciÃ³n.
- Herramientas de fine-tuning, inferencia, cuantizaciÃ³n y mÃ¡s.
- Una comunidad colaborativa activa de investigadores, empresas y desarrolladores.

**Usos comunes:**
- Integrar LLMs como LLaMA, Mistral o Phi en apps.
- Entrenar modelos personalizados.
- Evaluar y comparar arquitecturas de IA.
- Desplegar servicios de inferencia desde Hugging Face Inference Endpoints.
- Explorar nuevos modelos generativos, como Diffusion Models para imagen y audio.

---

## ğŸ§° Ollama

[**Ollama**](https://ollama.com) es una herramienta sencilla para ejecutar modelos LLM localmente, con soporte para mÃºltiples modelos open-source preconfigurados.

### Requisitos

- macOS, Linux o Windows (con soporte WSL)
- Docker NO es necesario
- CPU moderna o GPU con soporte para aceleraciÃ³n (opcional)

### ğŸ“– DocumentaciÃ³n y APIs

- [Repositorio de GitHub](https://github.com/ollama/ollama)
- [DocumentaciÃ³n oficial de Ollama](https://github.com/ollama/ollama/tree/main/docss)
- [GuÃ­a de la API de Ollama](../../wiki/ollama_api)  
- [SDK de Python para Ollama](../../wiki/ollama_python_sdk)
- [CreaciÃ³n y uso de Modelfile en Ollama](../../wiki/ollama_modelfile)

---

## ğŸ–¥ï¸ LM Studio

[LM Studio](https://lmstudio.ai/) es una aplicaciÃ³n de escritorio que permite interactuar con modelos de lenguaje de manera local y visual. Funciona como una interfaz grÃ¡fica para Ollama y llama.cpp, y estÃ¡ pensada para usuarios no tÃ©cnicos o que prefieren una experiencia similar a ChatGPT, pero con modelos locales.

**CaracterÃ­sticas destacadas:**
- Descargar y ejecutar modelos directamente desde la app.
- Soporte para mÃºltiples modelos open-source (LLaMA, Mistral, Phi, etc.).
- PersonalizaciÃ³n de temperatura, longitud de respuesta y formato.
- Compatible con macOS, Windows y Linux.

Ideal para experimentaciÃ³n, redacciÃ³n de contenido, aprendizaje y evaluaciÃ³n rÃ¡pida de modelos sin escribir cÃ³digo.

---

## ğŸ§© AnythingLLM

[AnythingLLM](https://anythingllm.com/) es una plataforma de cÃ³digo abierto que permite crear un **asistente privado impulsado por LLMs**, capaz de trabajar con tus propios datos, documentos y fuentes de conocimiento.  
[ğŸ™ GitHub Repository](https://github.com/Mintplex-Labs/anything-llm)

**Â¿QuÃ© puedes hacer con AnythingLLM?**
- Subir archivos (PDF, DOCX, TXT, etc.) y hacer preguntas sobre su contenido.
- Conectar fuentes externas como Notion, GitHub repos, sitios web y mÃ¡s.
- Utilizar diferentes backends como OpenAI, Ollama, Mistral, Hugging Face, entre otros.
- Desplegarlo en local, en servidores personales o en la nube.
- Administrar mÃºltiples espacios y usuarios con control de acceso.

Una soluciÃ³n ideal para construir un **chat corporativo privado**, motores de bÃºsqueda internos o asistentes personales de conocimiento con arquitectura plug-and-play.

---

## ğŸ“š Msty

[Msty](https://msty.app/) es una herramienta especializada en la **visualizaciÃ³n y anÃ¡lisis interno** de modelos de lenguaje.  
[ğŸ™ GitHub Repository](https://github.com/linonetwo/msty)

Te permite explorar:
- **Tokens** generados en cada paso
- **Embeddings** vectoriales y distancias semÃ¡nticas
- **Probabilidades de predicciÃ³n** token a token

Ideal para investigadores, educadores y quienes quieren entender cÃ³mo "piensa" un modelo. Msty puede ayudarte a explicar errores, mejorar prompts o hacer debugging de outputs inesperados.

---

## ğŸŒ Gradio

[Gradio](https://www.gradio.app/) es una librerÃ­a de Python que permite construir interfaces web para modelos de machine learning y deep learning en minutos. Ideal para prototipos, demos y validaciÃ³n con usuarios.

**Usos principales:**
- Crear formularios o chatbots con LLMs.
- Visualizar outputs de modelos de imagen, audio o NLP.
- Integrar con Hugging Face Spaces o notebooks.

Con unas pocas lÃ­neas de cÃ³digo puedes desplegar una interfaz intuitiva y compartible con cualquier persona.

---

## ğŸ“Š Streamlit

[Streamlit](https://streamlit.io) es un framework en Python para crear **aplicaciones web interactivas de forma rÃ¡pida**, ideal para prototipar interfaces con modelos de IA.

### ğŸ”§ CaracterÃ­sticas clave
- Interfaz muy sencilla basada en Python puro (sin necesidad de HTML/CSS/JS).
- Ideal para dashboards, demos de modelos y visualizaciÃ³n de datos.
- IntegraciÃ³n directa con librerÃ­as como `pandas`, `plotly`, `matplotlib` y APIs de LLMs.
- Permite desplegar aplicaciones fÃ¡cilmente en la nube mediante [Streamlit Community Cloud](https://streamlit.io/cloud).

---

## ğŸ—£ï¸ ElevenLabs

[ElevenLabs](https://elevenlabs.io) es una plataforma lÃ­der en **generaciÃ³n de voz mediante IA**.  
Permite crear voces sintÃ©ticas realistas en mÃºltiples idiomas y estilos, siendo ampliamente usada para aplicaciones de:

- **NarraciÃ³n de audiolibros y podcasts.**
- **GeneraciÃ³n de diÃ¡logos en videojuegos.**
- **ConversiÃ³n de texto a voz (TTS) en asistentes virtuales.**
- **CreaciÃ³n de personajes con voces personalizadas.**

### ğŸ”§ CaracterÃ­sticas clave
- Modelos de voz de alta fidelidad y expresividad.
- Soporte multilingÃ¼e y clonaciÃ³n de voz.
- API sencilla para integraciones en aplicaciones web y mÃ³viles.
- Planes de uso gratuito y de pago segÃºn volumen de caracteres.

### ğŸ“š LibrerÃ­as y SDKs
- [ElevenLabs Python SDK](https://pypi.org/project/elevenlabs/)
- API REST para integraciÃ³n con cualquier lenguaje.
- Plugins y conectores para aplicaciones de contenido multimedia.

---

## ğŸ”— Recursos adicionales

### ğŸ“š DocumentaciÃ³n y Comparativas

- [ğŸ¤— Hugging Face Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) â€“ Comparativa actualizada de modelos open-source.
- [ğŸ§  Awesome Open LLMs](https://github.com/Hannibal046/Awesome-LLMs) â€“ Lista curada de modelos, datasets y herramientas.
- [ğŸ¤— Hugging Face Hub](https://huggingface.co/models) â€“ Repositorio central de modelos preentrenados y datasets para IA generativa, NLP, visiÃ³n y mÃ¡s.

### âš™ï¸ Frameworks y LibrerÃ­as
- [ğŸ“ˆ Pydantic](https://docs.pydantic.dev/latest/) - La librerÃ­a de validaciÃ³n de datos mÃ¡s utilizada en Python
    - [ğŸ™ GitHub Repository](https://github.com/pydantic/pydantic)
- [ğŸ§ª LangChain](https://www.langchain.com/) â€“ OrquestaciÃ³n de agentes y flujos con LLMs, APIs y herramientas externas.
- [ğŸ” LangGraph](https://www.langgraph.dev/) â€“ Framework para flujos conversacionales multiestado con LLMs.
- [ğŸ“¦ LlamaIndex](https://www.llamaindex.ai/) â€“ Framework para crear aplicaciones de RAG (Retrieval-Augmented Generation).
- [ğŸ¤— Transformers (Hugging Face)](https://huggingface.co/docs/transformers/index) â€“ LibrerÃ­a para el uso de modelos de lenguaje en Python.

### Personas de Interes
- [Jeremy Howard](https://jeremy.fast.ai/)
    - [ğŸ™ GitHub Repository](https://github.com/jph00)
    - [ğŸ™ Fast.ai GitHub Repository](https://github.com/fastai)
    - [fast.aiâ€”Making neural nets uncool again](https://www.fast.ai/)
- [Maxime Labonne](https://mlabonne.github.io/blog/)
    - [ğŸ™ GitHub Repository](https://github.com/mlabonne)
    - [ğŸ¤— Hugging Face Page](https://huggingface.co/mlabonne)
    - [LLM Engineer's Handbook](https://github.com/PacktPublishing/LLM-Engineers-Handbook)
- [Colin Kealty (Bartowski)](https://x.com/bartowski1182)
    - [ğŸ™ GitHub Repository](https://github.com/bartowski1182)
    - [ğŸ¤— Hugging Face Page](https://huggingface.co/bartowski)
    - [ğŸ¤— Hugging Face LM Studio Community Page](https://huggingface.co/lmstudio-community)
- [David Kim](https://x.com/interpreter_ai)
    - [ğŸ™ GitHub Repository](https://github.com/davidkimai)
    - [ğŸ¤— Hugging Face Recursive Labs Page](https://huggingface.co/recursivelabsai)
- [Drew Breunig](https://www.dbreunig.com/)



dspy - .inspect_history()

ShareGPT Format
Alpaca Format (Llama)